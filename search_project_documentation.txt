High level goal:
High level is the goal to build web based search tool that enables vector search over books. The target user is
currently people like my girlfriend -- fantasy / sci-fi readers.

Core components:
1. Chunking the book into smaller parts for embedding model to process including some metadata (chunk index).
2. Embedding the books.
3. Vector search user query (retrieve top K chunks).
4. LLM refinement (using top K chunks + query) to select the single best chunk.
---
5. Web application (Flask + HTML/CSS/JS) for user interface.
6. Feedback mechanism (thumbs up/down on results, saved to JSON).
---
7. Hosting for the website.

---------
Manual Chunker (`manual_chunker.py`):
This script processes a PDF book file specified in its configuration (currently hardcoded list).
It extracts all text content using `pdfplumber`.
The text is cleaned (excessive newlines/spaces removed).
It uses `langchain` and `tiktoken` (v0.9.0+) to split text into chunks (target size, overlap).
For each chunk, it saves its index (0-based) and text.
The output is a JSON file per book (e.g., `Promise of Blood_full.json`).
Each entry contains: `chunk_index`, `text`.
Note: Input file handling could be made more dynamic (e.g., command-line args).

---
Embed Chunks (`embed_chunks.py`):
This script generates vector embeddings for book chunks using the Cohere API (v5.x, `ClientV2`).
It processes output JSON from the chunker (hardcoded list of `*_full.json` files).
It loads the Cohere API key from `.env`.
Initializes the Cohere client.
Loads chunked data, extracts texts, batches them, and generates embeddings (`embed-v4.0`, float).
Combines original chunk data (index, text) with the generated embedding vector.
Saves the combined data to a new output JSON file per book (e.g., `promise_of_blood_embeded.json`).
Note: Input file handling could be made more dynamic.

---
Search Chunks (`search_chunks.py`):
This script provides the `search_book_chunks` function used by the web application.
It performs semantic search followed by LLM refinement to find the single best chunk.
Steps:
- Loads Cohere and DeepSeek API keys from `.env`.
- Initializes Cohere (`ClientV2`) and DeepSeek (`openai` library) clients.
- Takes user query and path to a specific book's embedding file as input.
- Loads the specified book's embedding data.
- Generates a query embedding (Cohere).
- Performs vector search (dot product) to find the top N (currently 15) semantically similar chunks.
- Formats the text of these top N chunks (identified by `chunk_index`) for the LLM.
- Sends the query and formatted top N chunks to the DeepSeek chat model.
- Prompts the LLM for step-by-step reasoning and selection of the single best `chunk_index`.
- Parses the LLM response robustly using regex:
    - Searches the entire response for the last occurrence of `Final Answer: chunk XX` (case-insensitive).
    - Handles explicit `Final Answer: chunk none`.
    - Extracts the integer chunk index.
- Finds the data (including text *from the embedding file*) for the LLM-selected chunk index
  within the top N semantic results initially retrieved.
- Returns a list containing the data dictionary for the single chunk selected by the LLM (or the
  top semantic result as fallback). This dictionary contains the `chunk_index` and text derived
  from the `embedded_books/` JSON.
- **Important**: The `app.py` backend uses this result's `chunk_index` to load the actual display
  text from the corresponding file in `chunks_json/`.

---
Web Application (`app.py` & `templates/index.html`):
The web application uses Flask to serve a single-page interface (`index.html`) styled with Tailwind CSS.
Key features:
- Header with application title.
- Dynamic Book Selection: Buttons are generated via JavaScript based on `*_embeded.json` files found
  in the `embedded_books/` directory.
- Search Bar & Button: Input field for queries and an explicit Search button. These are disabled
  until a book is selected.
- Book Selection Prompt: A message prompts the user to select a book if none is active.
- Search Trigger: Search is initiated by pressing Enter in the input field or clicking the button.
- Loading Indicator: A spinner and text appear while the backend processes the search.
- Result Display & Navigation:
    - Shows the single text chunk returned by the backend.
    - The backend (`app.py` /search route) takes the `chunk_index` from `search_chunks`, reads
      the text from the corresponding file in `chunks_json/`, and sends this text and the total
      chunk count for the book to the frontend.
    - "Previous Chunk" and "Next Chunk" buttons appear below the result text.
    - Clicking these buttons calls a new backend endpoint (`/get_chunk_text`) to fetch and display
      the text of the adjacent chunk without running a new search.
    - Buttons are disabled at the start/end of the book.
- Feedback:
    - Solid thumbs up/down buttons appear next to the main "Result" heading when a result is shown.
    - Clicking either button sends the current `query`, `book_filename`, `chunk_index`, and
      correctness (true/false) to the backend `/feedback` endpoint.
    - The buttons become disabled, visually change color, and are replaced by a "Feedback Sent!"
      message after a short delay.

Backend (`app.py`):
- Serves the main `index.html` page, passing the list of available books.
- Provides a `/search` endpoint (POST) that:
    - Receives the `query` and selected `book_filename` as JSON.
    - Validates input and constructs the full path to the embedding file.
    - Calls the `search_book_chunks` function from `search_chunks.py`.
    - Takes the single result dictionary (containing `chunk_index`, text from embedding file, etc.)
      returned by the search function.
    - **Crucially**, uses the returned `chunk_index` to open the corresponding original chunk file
      in `chunks_json/` and reads the `text` field from that file.
    - Creates a new result dictionary containing this text from `chunks_json/` and other relevant
      metadata (`chunk_index`, `score`, etc.).
    - Returns this final result data as JSON to the frontend.
    - Includes basic logging and error handling.

- Provides a `/get_chunk_text` endpoint (POST) that:
    - Receives `book_filename` and `chunk_index`.
    - Reads the specified chunk's text from the corresponding `chunks_json/` file.
    - Returns the chunk data (text, index) as JSON.

- Provides a `/feedback` endpoint (POST) that:
  - Receives `query`, `book_filename`, `chunk_index`, and `is_correct` as JSON.
  - Validates the input data.
  - Loads existing feedback data from `feedback_data.json`.
  - Appends the new feedback entry to the list.
  - Saves the updated list back to `feedback_data.json`.
  - Returns a success message.

Frontend (`index.html` JavaScript):
- Dynamically creates book selection buttons based on data passed from Flask.
- Handles book selection state (only one active), updates search bar placeholder, enables/disables
  search input/button, and shows/hides the 'Select Book' prompt.
- Contains `triggerSearch` function called on Enter/button click:
    - Validates that a book is selected and a query exists.
    - Shows loading indicator, disables inputs.
    - Sends `query` and `book_filename` to the `/search` backend endpoint using `fetch`.
    - Calls `displayResults` with the response.
    - Hides loading indicator, re-enables inputs in a `finally` block.
- Contains `displayResults` function:
    - Clears previous results/loading indicator.
    - Handles 'No results found' or error messages.
    - Clones a hidden HTML template for the result card.
    - Populates the card with the text, book title, and chunk index.
    - Initializes state for navigation: remembers the original chunk index (`originalChunkIndex`)
      and resets a cache for full chunk text (`chunkFullTextCache`). The original chunk's full
      text is added to the cache.
    - Shows the global feedback buttons next to the "Result" heading.
    - Appends the card to the results area.
    - Sets up event listeners for the "Previous Chunk" and "Next Chunk" buttons, enabling/
      disabling them based on the current index and total chunk count.

- Contains `getFullChunkText` helper async function:
    - Checks if the requested chunk index's full text is already in `chunkFullTextCache`.
    - If cached, returns the cached text.
    - If not cached, calls the `/get_chunk_text` endpoint to fetch the full text.
    - Stores the fetched full text in the cache before returning it.

- Contains `fetchChunkText` async function:
    - Called by navigation buttons ('previous' or 'next').
    - Uses `getFullChunkText` to retrieve the full text of the target chunk.
    - Implements specific overlap trimming logic:
        - If the target chunk *is* the `originalChunkIndex`, its full text is always displayed.
        - If navigating 'next' (from N to N+1), uses `getFullChunkText` to get chunk N's text
          and displays chunk N+1 after removing overlap: `removeOverlap(textN, textN+1)`.
        - If navigating 'previous' (from N to N-1), uses `getFullChunkText` to get chunk N-2's
          text and displays chunk N-1 after removing overlap: `removeOverlap(textN-2, textN-1)`.
    - Updates the result card content with the (potentially trimmed) text and target index.
    - Updates the `currentChunkIndex` and stores the *full* target text in `currentFullChunkText`.
    - Updates the navigation button states.

- Contains global feedback button listeners (setup on page load):
    - Added to the thumbs up/down buttons next to the "Result" heading.
    - When clicked, call the `sendFeedback` function.

- Contains `sendFeedback` async function:
  - Reads current `query`, `chunk_index`, and `book_filename` from the page state.
  - Sends a POST request with the data to the `/feedback` endpoint.
  - Handles the response and potential errors, updating the UI (disabling buttons, showing message).

Dependencies are managed through `requirements.txt` (including `flask`, `cohere`, `openai`, `numpy`,
`python-dotenv`, etc.) and API keys via `.env`.

To run the web application locally:
1. Ensure Python 3.13+ and pip are installed.
2. Create and activate a virtual environment (e.g., `python3 -m venv venv`, `source venv/bin/activate`).
3. Install dependencies: `pip install -r requirements.txt`.
4. Create a `.env` file with your `COHERE_API_KEY` and `DEEPSEEK_API_KEY`.
5. Ensure embedding files (`*_embeded.json`) are present in the `embedded_books/` directory.
6. Run the Flask app: `python app.py`.
7. Access the application at `http://localhost:5000`.

For production deployment:
1. The application can be deployed on any WSGI-compatible server (e.g., Gunicorn).
2. Recommended hosting options: Heroku, DigitalOcean App Platform, AWS, Google Cloud.
3. Environment variables must be configured for production.
4. Use a production-grade WSGI server.
5. Set up proper logging and monitoring.
6. Configure HTTPS.

Future Expansion:
- Implement backend logic to handle and store user feedback.
- Analyze and utilize the collected feedback data (from `feedback_data.json`).
- Improve error handling and user messaging.
- Make input file handling more dynamic in `manual_chunker.py` and `embed_chunks.py`.
- Potentially add user authentication and book management features.

