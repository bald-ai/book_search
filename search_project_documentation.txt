High level goal:
High level is the goal to build web based search tool that enables vector search over books. The target user is
currently people like my girlfriend -- fantasy / sci-fi readers.

Core components:
1. Chunking the book into smaller parts for embedding model to process including some metadata (chunk index).
2. Embedding the books.
3. Vector search user query (retrieve top K chunks).
4. LLM refinement (using top K chunks + query) to select the single best chunk.
---
5. Web application (Flask + HTML/CSS/JS) for user interface.
6. Feedback mechanism (thumbs up/down on results, saved to JSON).
---
7. Hosting for the website.

---------
Manual Chunker (`manual_chunker.py`):
This script processes a PDF book file specified in its configuration (currently hardcoded list).
It extracts all text content using `pdfplumber`.
The text is cleaned (excessive newlines/spaces removed).
It uses `langchain` and `tiktoken` (v0.9.0+) to split text into chunks (target size, overlap).
For each chunk, it saves its index (0-based) and text.
The output is a JSON file per book (e.g., `Promise of Blood_full.json`).
Each entry contains: `book_name` (derived from filename), `chunk_index`, `text`.
Note: Input file handling could be made more dynamic (e.g., command-line args).

---
Embed Chunks (`embed_chunks.py`):
This script generates vector embeddings for book chunks using the Cohere API (v5.x, `ClientV2`).
It processes output JSON from the chunker (hardcoded list of `*_full.json` files).
It loads the Cohere API key from `.env`.
Initializes the Cohere client.
Loads chunked data, extracts texts, batches them, and generates embeddings (`embed-v4.0`, float).
Combines original chunk data (index, text) with the generated embedding vector.
Saves the combined data to a new output JSON file per book (e.g., `promise_of_blood_embeded.json`).
The structure for each entry is: `book_name`, `chunk_index`, `text`, `embedding`.
Note: Input file handling could be made more dynamic.

---
Search Chunks (`search_chunks.py`):
This script provides the asynchronous `search_book_chunks` function used by the web application.
It now supports searching across *multiple* selected books using a concurrent, two-stage
LLM refinement process.

Stage 1: Concurrent Refinement per Book
- Takes user query and a *list* of paths to selected books' embedding files.
- Loads Cohere and DeepSeek API keys from `.env`.
- Creates per-request asynchronous clients for Cohere (`AsyncClient`) and DeepSeek (`AsyncOpenAI`) using `async with` for proper cleanup.
- **Concurrently** for each selected book (`asyncio.gather`):
    - Loads only that book's embedding data.
    - Generates a query embedding (async Cohere).
    - Performs vector search (dot product) to find the top N (default 15) semantic matches.
    - Loads the *full text* for these top N chunks from the corresponding `chunks_json/` file.
    - Sends the query and formatted top N chunk *texts* to the DeepSeek chat model (async).
    - Prompts the LLM for step-by-step reasoning and selection of the single best `chunk_index`
      *for that specific book*.
    - Parses the LLM response robustly (handles `None`, errors). If LLM fails/selects `None`,
      it falls back to the top semantic result (rank 0) for that book.
    - Stores the data dictionary of the selected candidate chunk (book name, chunk index, text
      from embedding file, score, rank within the book's semantic results).

Stage 2: Final Aggregation and Refinement (if >1 book searched)
- Collects the best candidate data dictionaries from all successful Stage 1 tasks.
- If only one book was searched, its Stage 1 result is returned directly.
- If multiple candidates exist:
    - Loads the *full text* for each candidate chunk from its respective `chunks_json/` file.
    - Creates a new prompt containing the original user query and the texts of all candidate
      chunks (clearly identified by book and index).
    - Sends this combined prompt to the DeepSeek chat model in a *single*, final async call.
    - Prompts the LLM to select the *overall best* candidate chunk from the provided list.
    - Parses the response. If the LLM fails or provides an invalid choice, it falls back to the
      candidate that had the best rank (and score as tie-breaker) from Stage 1.

Output:
- Returns a list containing the single data dictionary for the final selected chunk.
- This dictionary includes `book_name`, `chunk_index`, `text` (from the embedding file),
  `score` (from Stage 1 semantic search), and `rank` (from Stage 1 semantic search).
- **Important**: `app.py` uses the returned `book_name` and `chunk_index` to load the actual
  display text from the corresponding `chunks_json/` file. The subfolder structure is preserved.
- Includes improved error handling for API calls (checks response structure, logs HTML errors).
- Clears and writes to `last_llm_output.txt` at the start of each
  `search_book_chunks` call, recording only the LLM outputs for the latest query.
- Appends all raw LLM responses to `last_llm_output.txt`, including Stage 1 per-book
  responses (with book name and query context) and the final Stage 2 response.

---
Web Application (`app.py` & `templates/index.html`):
The web application uses Flask (with `async` support enabled) to serve a single-page interface.
Key features:
- Header with application title.
- Dynamic Book Selection: Buttons are generated via JavaScript based on `*_embeded.json` files found
  recursively in the `embedded_books/` directory (including subfolders for trilogies/series).
- Search Bar & Button: Input field for queries and an explicit Search button. These are disabled
  until a book is selected.
- Book Selection Prompt: A message prompts the user to select a book if none is active.
- Search Trigger: Search is initiated by pressing Enter in the input field or clicking the button.
- Loading Indicator: A spinner and text appear while the backend processes the search.
- Result Display & Navigation:
    - Shows the single text chunk returned by the backend.
    - The backend (`app.py` /search route) takes the `chunk_index` from `search_chunks`, reads
      the text from the corresponding file in `chunks_json/` (using the same subfolder structure as in embedded_books/), and sends this text and the total
      chunk count for the book to the frontend.
    - The backend now always includes the full relative `book_filename` (including any series/subfolder)
      in the search result. The frontend uses this exact `book_filename` for all navigation and chunk
      fetches (such as next/previous chunk), instead of reconstructing it from the book name. This ensures
      correct handling of nested series folders for navigation and feedback.
    - "Previous Chunk" and "Next Chunk" buttons appear below the result text.
    - Clicking these buttons calls a new backend endpoint (`/get_chunk_text`) to fetch and display
      the text of the adjacent chunk without running a new search.
    - Buttons are disabled at the start/end of the book.
    - **Clipboard Copy Button:** A clipboard icon button appears at the top right of the chunk
      text box in the result card. Clicking this button copies the currently displayed chunk text
      to the user's clipboard. A small tooltip appears near the button to confirm the text was
      copied successfully.
    - **Search Timer:** After each search, the elapsed time (in seconds) is displayed to the right
      of the feedback buttons in the top Result heading. The timer is cleared when a new search
      starts or if an error occurs.
- Feedback:
    - Solid thumbs up/down buttons appear next to the main "Result" heading when a result is shown.
    - Clicking either button sends the current `query`, `book_filename`, `chunk_index`, and
      correctness (true/false) to the backend `/feedback` endpoint.
    - The buttons become disabled, visually change color, and are replaced by a "Feedback Sent!"
      message after a short delay.

Backend (`app.py`):
- Serves the main `index.html` page, passing the list of available books.
- Provides an *asynchronous* `/search` endpoint (POST) (`async def search`):
    - Receives the `query` and a *list* of selected `book_filenames` (relative paths, including subfolders).
    - Logs the search query and selected books list to `all_queries.json`.
    - Validates input and constructs full paths to the embedding files (using subfolder structure).
    - Calls and *awaits* the `search_book_chunks` async function from `search_chunks.py`.
    - Takes the single result dictionary returned by the search function.
    - **Crucially**, uses the returned `book_name` and `chunk_index` to open the corresponding
      original chunk file in `chunks_json/` (using the same subfolder structure as in embedded_books/) and reads the `text` field from that file.
    - Creates a new result dictionary containing this text from `chunks_json/` and other relevant
      metadata (`chunk_index`, `score`, `rank`, `total_chunks` for that book).
    - Returns this final result data as JSON to the frontend.
    - Includes basic logging and error handling.

- Provides a `/get_chunk_text` endpoint (POST) that:
    - Receives `book_filename` (relative path, including subfolders) and `chunk_index`.
    - Reads the specified chunk's text from the corresponding `chunks_json/` file (using the same subfolder structure as in embedded_books/).
    - Returns the chunk data (text, index) as JSON.

- Provides a `/feedback` endpoint (POST) that:
  - Receives `query`, `selected_books` (list of relative paths), `chunk_index`, `is_correct`, and `rank` as JSON.
  - The `rank` represents the 0-based position of the displayed chunk within the initial top N
    semantic search results before LLM refinement.
  - Validates the input data, including checking that `rank` is null or a non-negative integer.
  - Loads existing feedback data from `feedback_data.json`.
  - Appends the new feedback entry (including the `rank`) to the list.
  - Saves the updated list back to `feedback_data.json`.
  - Returns a success message.

Frontend (`index.html` JavaScript):
- Users select books to search using a custom dropdown menu at the top of the search area.
- The dropdown displays all series (folders) as groups, each with a checkbox to select or deselect
  the entire series at once.
- Each book is listed under its series with its own checkbox, allowing for individual selection.
- Users can select any combination of series and/or individual books. This is a true multi-select
  interface: you can search across multiple series, a single series, or just a few books from any
  series.
- The dropdown label updates to show the current selection (series and/or books).
- The search input and logic use the union of all selected books for the search.
- The UI is dynamic: as you add new series or books to the backend, they will automatically appear
  in the dropdown menu, grouped by series.
- The old folder picker and toggle-all button have been replaced by this more flexible, nested
  multi-select dropdown.
- The book toggle area is now hidden, as all selection is handled via the dropdown.
- The rest of the search and feedback UI remains unchanged.

Dependencies are managed through `requirements.txt` (including `